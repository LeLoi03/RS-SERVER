# src/utils/data_loader.py

import pandas as pd
import requests
import io
from datetime import datetime

import config.config as config
from src.utils.live_logger import LiveLogger

def fetch_and_prepare_data(force_fetch: bool = False) -> pd.DataFrame:
    """
    Fetches feedback data from the API, processes it, and caches it locally.
    
    Args:
        force_fetch (bool): If True, always fetches from the API, ignoring the cache.
        
    Returns:
        pd.DataFrame: A DataFrame with the required columns for the pipeline.
    """
    # 1. Logic Caching: N·∫øu kh√¥ng √©p bu·ªôc v√† file cache t·ªìn t·∫°i, d√πng cache
    if not force_fetch and config.CACHED_DATASET_PATH.exists():
        LiveLogger.log(f"‚úÖ Using cached data from '{config.CACHED_DATASET_PATH.name}'.")
        return pd.read_csv(config.CACHED_DATASET_PATH)

    # 2. G·ªçi API ƒë·ªÉ l·∫•y d·ªØ li·ªáu m·ªõi
    LiveLogger.log(f"üîÑ Fetching fresh data from API: {config.FEEDBACKS_API_URL}")
    if not config.FEEDBACKS_API_URL:
        raise ValueError("FEEDBACKS_API_URL is not configured in .env file.")

    headers = {
        "Accept": "*/*",
        # Th√™m header Authorization n·∫øu API c·ªßa b·∫°n y√™u c·∫ßu
        "Authorization": f"Bearer {config.FEEDBACKS_API_KEY}"
    }

    try:
        response = requests.get(config.FEEDBACKS_API_URL, headers=headers, timeout=60) # 60s timeout
        response.raise_for_status() # B√°o l·ªói n·∫øu status code l√† 4xx ho·∫∑c 5xx
    except requests.RequestException as e:
        LiveLogger.log(f"‚ùå API call failed: {e}")
        raise ConnectionError(f"Could not fetch data from API. Error: {e}")

    # 3. X·ª≠ l√Ω v√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu
    LiveLogger.log("üõ†Ô∏è  Processing and transforming API data...")
    
    # ƒê·ªçc n·ªôi dung CSV t·ª´ response v√†o DataFrame
    csv_data = io.StringIO(response.text)
    df = pd.read_csv(csv_data)

    # √Ånh x·∫° t√™n c·ªôt t·ª´ API sang t√™n c·ªôt m√† pipeline s·ª≠ d·ª•ng
    column_mapping = {
        'creatorId': 'user_id',
        'conferenceId': 'conference_key',
        'description': 'review',
        'star': 'rating',
        'createdAt': 'timestamp'
    }
    df = df.rename(columns=column_mapping)

    # Chuy·ªÉn ƒë·ªïi c·ªôt 'timestamp' t·ª´ chu·ªói sang Unix timestamp (s·ªë nguy√™n)
    # ƒê·ªãnh d·∫°ng c·ªßa API: 'Sun Aug 31 2025 03:22:41 GMT+0000 (Coordinated Universal Time)'
    # Ch√∫ng ta c·∫ßn lo·∫°i b·ªè ph·∫ßn trong ngo·∫∑c ƒë∆°n ƒë·ªÉ pandas c√≥ th·ªÉ hi·ªÉu
    df['timestamp'] = df['timestamp'].str.replace(r' \(.*\)', '', regex=True)
    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%a %b %d %Y %H:%M:%S GMT%z')
    df['timestamp'] = (df['timestamp'].astype('int64') // 10**9).astype('int32')

    # Ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt c·∫ßn thi·∫øt cho pipeline
    required_columns = ['user_id', 'conference_key', 'review', 'rating', 'timestamp']
    df = df[required_columns]
    
    # 4. L∆∞u v√†o cache ƒë·ªÉ s·ª≠ d·ª•ng cho c√°c l·∫ßn ch·∫°y sau
    LiveLogger.log(f"üíæ Caching processed data to '{config.CACHED_DATASET_PATH.name}'...")
    config.CACHED_DATASET_PATH.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(config.CACHED_DATASET_PATH, index=False)
    
    return df